{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Installation  \n",
    "\n",
    "This notebook requires specific dependencies, which are listed in `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent AI for Autonomous Astrophysics Research\n",
    "\n",
    "This notebook implements a system of AI agents that work together to detect anomalies in astrophysical data, with a focus on gravitational wave events. The system:\n",
    "\n",
    "1. Ingests data from astrophysics sources (GWOSC, NASA HEASARC)\n",
    "2. Detects anomalies in the data\n",
    "3. Generates theoretical models to explain the anomalies\n",
    "4. Correlates findings with existing research\n",
    "5. Visualizes the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Task, Crew, LLM, Process\n",
    "from langchain.tools import Tool, tool\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access API key\n",
    "# ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# if not ANTHROPIC_API_KEY:\n",
    "#     print(\"No Anthropic key found...\")\n",
    "# else:\n",
    "#     print(ANTHROPIC_API_KEY[:4],\"*********************\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"No OpenAI key found...\")\n",
    "else:\n",
    "    print(OPENAI_API_KEY[:4],\"*********************\")\n",
    "\n",
    "\n",
    "# Set environment variable for crewai\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Tools\n",
    "\n",
    "First, we'll create tools to fetch real astrophysics data from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Task, Crew, LLM, Process\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Ensure API key exists\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"No OpenAI API key found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LLM\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "openai_llm = LLM(model=openai_model, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOL: Fetch GWOSC Events\n",
    "@tool\n",
    "def fetch_gwosc_events(limit=5):\n",
    "    \"\"\"Fetch gravitational wave events from GWOSC.\"\"\"\n",
    "    try:\n",
    "        url = \"https://gwosc.org/eventapi/json/allevents/\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            events = list(data.get('events', {}).items())[:limit]\n",
    "            \n",
    "            formatted_events = [\n",
    "                {\n",
    "                    'event_id': event_id,\n",
    "                    'mass_1_source': event_data.get('mass_1_source'),\n",
    "                    'mass_2_source': event_data.get('mass_2_source'),\n",
    "                    'luminosity_distance': event_data.get('luminosity_distance'),\n",
    "                    'network_snr': event_data.get('network_snr')\n",
    "                }\n",
    "                for event_id, event_data in events\n",
    "            ]\n",
    "            return formatted_events\n",
    "        else:\n",
    "            return f\"Error fetching data: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\"\n",
    "\n",
    "# TOOL: Validate GWOSC Data\n",
    "@tool\n",
    "def validate_gwosc_data(events):\n",
    "    \"\"\"Validate event IDs and required fields before passing to the workflow.\"\"\"\n",
    "    valid_events = []\n",
    "    invalid_events = []\n",
    "    warnings = []\n",
    "\n",
    "    required_fields = ['mass_1_source', 'mass_2_source', 'luminosity_distance']\n",
    "    optional_fields = ['network_snr']\n",
    "    event_id_pattern = re.compile(r'^GW\\d{6}_\\d{6}-v\\d+$')  # Example: GW190521_030229-v1\n",
    "\n",
    "    for event in events:\n",
    "        event_id = event.get('event_id', '')\n",
    "        missing_fields = [field for field in required_fields if event.get(field) is None]\n",
    "        missing_optional = [field for field in optional_fields if event.get(field) is None]\n",
    "\n",
    "        if not event_id_pattern.match(event_id):\n",
    "            invalid_events.append({\"event_id\": event_id, \"reason\": \"Invalid format\"})\n",
    "        elif missing_fields:\n",
    "            invalid_events.append({\"event_id\": event_id, \"reason\": f\"Missing required fields: {', '.join(missing_fields)}\"})\n",
    "        elif missing_optional:\n",
    "            warnings.append({\"event_id\": event_id, \"warning\": f\"Missing optional fields: {', '.join(missing_optional)}\"})\n",
    "            valid_events.append(event)  # Allow it to pass with warning\n",
    "        else:\n",
    "            valid_events.append(event)\n",
    "\n",
    "    return {\"valid_events\": valid_events, \"warnings\": warnings, \"invalid_events\": invalid_events}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT: Data Ingestor\n",
    "data_ingestor = Agent(\n",
    "    role=\"Astrophysical Data Ingestor\",\n",
    "    goal=\"Retrieve astrophysics event data.\",\n",
    "    backstory=\"You fetch raw event data from public astrophysics sources.\",\n",
    "    verbose=True,\n",
    "    tools=[fetch_gwosc_events],\n",
    "    allow_delegation=False,\n",
    "    llm_model=openai_model\n",
    ")\n",
    "\n",
    "# AGENT: Data Validator\n",
    "data_validator = Agent(\n",
    "    role=\"Astrophysical Data Validator\",\n",
    "    goal=\"Validate event IDs and check for missing fields.\",\n",
    "    backstory=\"You specialize in validating astrophysical event data.\",\n",
    "    verbose=True,\n",
    "    tools=[validate_gwosc_data],\n",
    "    allow_delegation=False,\n",
    "    llm_model=openai_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Fetch Data\n",
    "data_ingestion_task = Task(\n",
    "    description=\"Fetch the latest gravitational wave event data.\",\n",
    "    agent=data_ingestor,\n",
    "    expected_output=\"A structured dataset of gravitational wave events.\",\n",
    "    async_execution=False\n",
    ")\n",
    "\n",
    "# TASK: Validate Data\n",
    "data_validation_task = Task(\n",
    "    description=\"Validate event IDs and check for missing fields before processing.\",\n",
    "    agent=data_validator,\n",
    "    expected_output=\"List of valid events and rejected events with reasons.\",\n",
    "    context=[data_ingestion_task],  # Must run after data ingestion\n",
    "    async_execution=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREW: Minimal Workflow\n",
    "astrophysics_crew = Crew(\n",
    "    agents=[data_ingestor, data_validator],\n",
    "    tasks=[data_ingestion_task, data_validation_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# RUN CREW\n",
    "try:\n",
    "    print(\"Starting minimal astrophysics workflow...\\n\")\n",
    "    results = astrophysics_crew.kickoff()\n",
    "    print(\"\\nWorkflow execution completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structured results from API response\n",
    "task_outputs = results.tasks_output\n",
    "\n",
    "# Initialize formatted results\n",
    "formatted_results = \"\"\"\n",
    "## ANALYSIS REPORT  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Loop through task outputs and extract results\n",
    "for task in task_outputs:\n",
    "    agent_name = task.agent\n",
    "    task_result = task.raw if task.raw else \"No result available.\"\n",
    "\n",
    "    formatted_results += f\"\"\"\n",
    "\n",
    "## {agent_name} Analysis  \n",
    "\n",
    "{task_result}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save results as a structured text file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"astrophysics_report_{timestamp}.md\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(formatted_results)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [data_ingestor, data_validator]\n",
    "\n",
    "for agent in agents:\n",
    "    print(f\"{agent.role} is using LLM: {agent.llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
