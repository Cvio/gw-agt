{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Installation  \n",
    "\n",
    "This notebook requires specific dependencies, which are listed in `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent AI for Autonomous Astrophysics Research\n",
    "\n",
    "This notebook implements a system of AI agents that work together to detect anomalies in astrophysical data, with a focus on gravitational wave events. The system:\n",
    "\n",
    "1. Ingests data from astrophysics sources (GWOSC, NASA HEASARC)\n",
    "2. Detects anomalies in the data\n",
    "3. Generates theoretical models to explain the anomalies\n",
    "4. Correlates findings with existing research\n",
    "5. Visualizes the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-p *********************\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Task, Crew, LLM, Process\n",
    "from langchain.tools import Tool, tool\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access API key\n",
    "# ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# if not ANTHROPIC_API_KEY:\n",
    "#     print(\"No Anthropic key found...\")\n",
    "# else:\n",
    "#     print(ANTHROPIC_API_KEY[:4],\"*********************\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"No OpenAI key found...\")\n",
    "else:\n",
    "    print(OPENAI_API_KEY[:4],\"*********************\")\n",
    "\n",
    "\n",
    "# Set environment variable for crewai\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Tools\n",
    "\n",
    "First, we'll create tools to fetch real astrophysics data from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Validation function for event IDs and required fields\n",
    "def validate_gwosc_data(events):\n",
    "    \"\"\"Validate gravitational wave event data before passing to the workflow.\"\"\"\n",
    "    valid_events = []\n",
    "    invalid_events = []\n",
    "    \n",
    "    required_fields = ['mass_1_source', 'mass_2_source', 'luminosity_distance', 'network_snr']\n",
    "    event_id_pattern = re.compile(r'^GW\\d{6}_\\d{6}-v\\d+$')  # Example: GW190521_030229-v1\n",
    "\n",
    "    for event in events:\n",
    "        event_id = event.get('event_id', '')\n",
    "        missing_fields = [field for field in required_fields if event.get(field) is None]\n",
    "\n",
    "        if not event_id_pattern.match(event_id):\n",
    "            invalid_events.append({\"event_id\": event_id, \"reason\": \"Invalid format\"})\n",
    "        elif missing_fields:\n",
    "            invalid_events.append({\"event_id\": event_id, \"reason\": f\"Missing fields: {', '.join(missing_fields)}\"})\n",
    "        else:\n",
    "            valid_events.append(event)\n",
    "\n",
    "    return valid_events, invalid_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for fetching astrophysics data\n",
    "\n",
    "@tool\n",
    "def fetch_gwosc_events(limit=10):\n",
    "    \"\"\"Fetch gravitational wave events from GWOSC (Gravitational Wave Open Science Center).\"\"\"\n",
    "    try:\n",
    "        # GWOSC API endpoint for event data\n",
    "        url = f\"https://gwosc.org/eventapi/json/allevents/\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            events = list(data.get('events', {}).items())[:limit]\n",
    "            \n",
    "            # Format the events into a more readable structure\n",
    "            formatted_events = []\n",
    "            for event_id, event_data in events:\n",
    "                formatted_events.append({\n",
    "                    'event_id': event_id,\n",
    "                    'gps_time': event_data.get('GPS', None),\n",
    "                    'mass_1_source': event_data.get('mass_1_source', None),\n",
    "                    'mass_2_source': event_data.get('mass_2_source', None),\n",
    "                    'luminosity_distance': event_data.get('luminosity_distance', None),\n",
    "                    'network_snr': event_data.get('network_snr', None),\n",
    "                    'significant': event_data.get('significant', None),\n",
    "                    'instruments': event_data.get('instruments', None)\n",
    "                })\n",
    "            \n",
    "            return formatted_events\n",
    "        else:\n",
    "            return f\"Error fetching data: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def fetch_nasa_exoplanet_data(limit=10):\n",
    "    \"\"\"Fetch exoplanet data from NASA Exoplanet Archive.\"\"\"\n",
    "    try:\n",
    "        # API endpoint and query\n",
    "        base_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "\n",
    "        query = f\"SELECT TOP {limit} pl_name, pl_orbper, pl_rade, pl_bmasse, pl_orbeccen, st_spectype, discoverymethod, ra, dec FROM ps WHERE default_flag=1\"\n",
    "\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return f\"Error fetching NASA exoplanet data: {response.status_code} - {response.text}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_gw_strain_data(event_id):\n",
    "    \"\"\"Fetch strain data for a specific gravitational wave event.\"\"\"\n",
    "    try:\n",
    "        # Construct the URL for the strain data\n",
    "        url = f\"https://gwosc.org/eventapi/json/{event_id}/\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            event_data = response.json()\n",
    "            \n",
    "            # For demonstration purposes, we're returning the metadata\n",
    "            # In a real application, you would download the actual strain data from the provided URLs\n",
    "            return {\n",
    "                'event_id': event_id,\n",
    "                'data_details': event_data.get('data', {}),\n",
    "                'gps_time': event_data.get('GPS', None),\n",
    "                'detector_status': event_data.get('detectors', {})\n",
    "            }\n",
    "        else:\n",
    "            return f\"Error fetching strain data: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def fetch_astronomy_papers(keyword, limit=5):\n",
    "    \"\"\"Fetch relevant astronomy research papers from arXiv.\"\"\"\n",
    "    try:\n",
    "        # ArXiv API endpoint\n",
    "        base_url = \"http://export.arxiv.org/api/query?\"\n",
    "        query = f\"search_query=all:{keyword}+AND+cat:astro-ph&start=0&max_results={limit}\"\n",
    "        response = requests.get(base_url + query)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse the XML response to extract paper details\n",
    "            # For simplicity, we'll just extract titles and abstracts\n",
    "            from xml.etree import ElementTree as ET\n",
    "            \n",
    "            # Define namespace\n",
    "            namespace = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "            \n",
    "            # Parse XML\n",
    "            root = ET.fromstring(response.text)\n",
    "            \n",
    "            # Extract entries\n",
    "            papers = []\n",
    "            for entry in root.findall('atom:entry', namespace):\n",
    "                title = entry.find('atom:title', namespace).text.strip()\n",
    "                abstract = entry.find('atom:summary', namespace).text.strip()\n",
    "                published = entry.find('atom:published', namespace).text.strip()\n",
    "                link = entry.find('atom:id', namespace).text.strip()\n",
    "                \n",
    "                papers.append({\n",
    "                    'title': title,\n",
    "                    'abstract': abstract,\n",
    "                    'published': published,\n",
    "                    'link': link\n",
    "                })\n",
    "            \n",
    "            return papers\n",
    "        else:\n",
    "            return f\"Error fetching papers: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Tools\n",
    "\n",
    "Next, we'll create tools to analyze the data and detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for detecting anomalies in astrophysics data\n",
    "\n",
    "@tool\n",
    "def detect_gw_anomalies(events):\n",
    "    \"\"\"Detect anomalies in gravitational wave events.\"\"\"\n",
    "    anomalies = []\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    if not events or not isinstance(events, list):\n",
    "        return \"No valid events data provided.\"\n",
    "    \n",
    "    df = pd.DataFrame(events)\n",
    "    \n",
    "    # Check for missing columns\n",
    "    required_columns = ['mass_1_source', 'mass_2_source', 'luminosity_distance', 'network_snr']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            return f\"Required column {col} missing from events data.\"\n",
    "    \n",
    "    # Convert columns to numeric, handling non-numeric values\n",
    "    for col in required_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Example anomaly detection logic:\n",
    "    # 1. Unusually high mass ratio (mass_1/mass_2 > 10)\n",
    "    df['mass_ratio'] = df['mass_1_source'] / df['mass_2_source']\n",
    "    high_mass_ratio = df[df['mass_ratio'] > 10]\n",
    "    \n",
    "    # 2. Unusually high SNR events (signal-to-noise ratio)\n",
    "    high_snr = df[df['network_snr'] > df['network_snr'].quantile(0.9)]\n",
    "    \n",
    "    # 3. Unusually distant events\n",
    "    distant_events = df[df['luminosity_distance'] > df['luminosity_distance'].quantile(0.9)]\n",
    "    \n",
    "    # Compile anomalies\n",
    "    if not high_mass_ratio.empty:\n",
    "        for _, row in high_mass_ratio.iterrows():\n",
    "            anomalies.append({\n",
    "                'event_id': row['event_id'],\n",
    "                'anomaly_type': 'High mass ratio',\n",
    "                'details': f\"Mass ratio: {row['mass_ratio']:.2f} (mass_1: {row['mass_1_source']}, mass_2: {row['mass_2_source']})\",\n",
    "                'significance': 'high' if row['mass_ratio'] > 20 else 'medium'\n",
    "            })\n",
    "    \n",
    "    if not high_snr.empty:\n",
    "        for _, row in high_snr.iterrows():\n",
    "            anomalies.append({\n",
    "                'event_id': row['event_id'],\n",
    "                'anomaly_type': 'High SNR',\n",
    "                'details': f\"SNR: {row['network_snr']:.2f}\",\n",
    "                'significance': 'high' if row['network_snr'] > df['network_snr'].quantile(0.95) else 'medium'\n",
    "            })\n",
    "    \n",
    "    if not distant_events.empty:\n",
    "        for _, row in distant_events.iterrows():\n",
    "            anomalies.append({\n",
    "                'event_id': row['event_id'],\n",
    "                'anomaly_type': 'Distant event',\n",
    "                'details': f\"Distance: {row['luminosity_distance']:.2f} Mpc\",\n",
    "                'significance': 'high' if row['luminosity_distance'] > df['luminosity_distance'].quantile(0.95) else 'medium'\n",
    "            })\n",
    "    \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Modeling Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for theoretical modeling\n",
    "\n",
    "@tool\n",
    "def generate_theoretical_model(anomaly):\n",
    "    \"\"\"Generate a theoretical model to explain an observed anomaly.\"\"\"\n",
    "    if not anomaly:\n",
    "        return \"No anomaly data provided.\"\n",
    "    \n",
    "    # Examples of theoretical models based on anomaly type\n",
    "    anomaly_type = anomaly.get('anomaly_type', '')\n",
    "    event_id = anomaly.get('event_id', 'unknown')\n",
    "    details = anomaly.get('details', '')\n",
    "    \n",
    "    if 'High mass ratio' in anomaly_type:\n",
    "        return {\n",
    "            'event_id': event_id,\n",
    "            'model_name': 'Hierarchical Merger Model',\n",
    "            'description': 'This event may represent a hierarchical merger, where one of the black holes is itself a product of a previous merger. This explains the unusually high mass ratio.',\n",
    "            'key_parameters': {\n",
    "                'expected_spin': 'High',\n",
    "                'expected_precession': 'Yes',\n",
    "                'expected_eccentricity': 'Moderate to high'\n",
    "            },\n",
    "            'testable_predictions': [\n",
    "                'The larger black hole should have high spin parameter',\n",
    "                'Waveform should show signs of precession',\n",
    "                'May be associated with a globular cluster or active galactic nucleus'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    elif 'High SNR' in anomaly_type:\n",
    "        return {\n",
    "            'event_id': event_id,\n",
    "            'model_name': 'Binary Black Hole Optimal Orientation Model',\n",
    "            'description': 'This event may have an unusually high SNR due to optimal orientation with respect to our detectors, or due to gravitational lensing magnifying the signal.',\n",
    "            'key_parameters': {\n",
    "                'inclination_angle': 'Near 0° (face-on)',\n",
    "                'polarization': 'Aligned with detector',\n",
    "                'possible_lensing': 'Yes'\n",
    "            },\n",
    "            'testable_predictions': [\n",
    "                'Parameter estimation should favor face-on inclination',\n",
    "                'If lensed, should see multiple images with time delay',\n",
    "                'Higher-order modes should be suppressed relative to quadrupole'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    elif 'Distant event' in anomaly_type:\n",
    "        return {\n",
    "            'event_id': event_id,\n",
    "            'model_name': 'Primordial Black Hole Model',\n",
    "            'description': 'This distant event may involve primordial black holes formed in the early universe rather than from stellar collapse, explaining its unusual distance and potentially other parameters.',\n",
    "            'key_parameters': {\n",
    "                'formation_time': 'Early universe',\n",
    "                'expected_mass_distribution': 'Scale-invariant',\n",
    "                'expected_redshift': 'Higher than typical stellar-mass BBH'\n",
    "            },\n",
    "            'testable_predictions': [\n",
    "                'Mass should fall within specific peaks predicted by primordial formation models',\n",
    "                'Spins should be low',\n",
    "                'Should be no electromagnetic counterpart'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        return {\n",
    "            'event_id': event_id,\n",
    "            'model_name': 'Generic Anomaly Model',\n",
    "            'description': f'This event shows anomalous behavior described as: {details}. Further analysis required.',\n",
    "            'key_parameters': {\n",
    "                'unknown_parameter_1': 'Unknown',\n",
    "                'unknown_parameter_2': 'Unknown'\n",
    "            },\n",
    "            'testable_predictions': [\n",
    "                'Further data collection required to make specific predictions'\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the AI Agent Crew\n",
    "\n",
    "Now we'll define our AI agents and their tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AI agents\n",
    "\n",
    "# Set your LLM model here\n",
    "llm_model = \"claude-3-7-sonnet-thinking\"  # Replace with your preferred model\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "\n",
    "# Not Available: o1, o3-mini\n",
    "# Available: gpt-4.5-preview, gpt-4o-mini, gpt-4o\n",
    "\n",
    "openai_llm = LLM(\n",
    "    model=openai_model,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_base=\"https://api.openai.com/v1\"\n",
    ")\n",
    "# Define Data Validator Agent\n",
    "data_validator = Agent(\n",
    "    role=\"Astrophysical Data Validator\",\n",
    "    goal=\"Ensure all event IDs are correctly formatted and data fields are complete.\",\n",
    "    backstory=\"You specialize in validating astrophysical event data before further processing.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "# Data Ingestor - Focus on raw data retrieval only\n",
    "data_ingestor = Agent(\n",
    "    role=\"Astrophysical Data Ingestor\",\n",
    "    goal=\"Retrieve and structure real-time astrophysics data from various sources.\",\n",
    "    backstory=\"You specialize in collecting astrophysical data and structuring it for analysis. Your role is purely to gather and preprocess raw data.\",\n",
    "    verbose=True,\n",
    "    tools=[fetch_gwosc_events, fetch_nasa_exoplanet_data, fetch_gw_strain_data],\n",
    "    allow_delegation=True,\n",
    "    llm_model=openai_model\n",
    ")\n",
    "\n",
    "# Gravitational Wave Analyst - Extracts features & prepares for anomaly detection\n",
    "gravitational_wave_analyst = Agent(\n",
    "    role=\"Gravitational Wave Analyst\",\n",
    "    goal=\"Analyze gravitational wave data, extract key parameters, and prepare structured datasets.\",\n",
    "    backstory=\"You specialize in waveform analysis and astrophysical event characterization. Your job is to transform raw data into structured information for anomaly detection.\",\n",
    "    verbose=True,\n",
    "    tools=[fetch_gw_strain_data],  # Removed data fetching duplication\n",
    "    allow_delegation=True,\n",
    "    llm_model=openai_model\n",
    ")\n",
    "\n",
    "# Anomaly Detector - Now has access to astrophysical literature\n",
    "anomaly_detector = Agent(\n",
    "    role=\"Anomaly Detector\",\n",
    "    goal=\"Identify astrophysical anomalies while considering known astrophysical principles.\",\n",
    "    backstory=\"You specialize in detecting anomalies in astrophysical datasets. You reference both raw data and existing research to distinguish false positives.\",\n",
    "    verbose=True,\n",
    "    tools=[detect_gw_anomalies, fetch_astronomy_papers],  # Added research tool for validation\n",
    "    allow_delegation=True,\n",
    "    llm_model=openai_model\n",
    ")\n",
    "\n",
    "# Theoretical Modeler - Now references research & anomaly classification before generating models\n",
    "theoretical_modeler = Agent(\n",
    "    role=\"Theoretical Model Generator\",\n",
    "    goal=\"Develop scientifically sound explanations for astrophysical anomalies based on known physics.\",\n",
    "    backstory=\"You specialize in theoretical astrophysics, formulating models to explain unusual astrophysical phenomena. You validate proposed explanations against existing research and anomaly classifications.\",\n",
    "    verbose=True,\n",
    "    tools=[generate_theoretical_model, fetch_astronomy_papers],  # Stays the same but benefits from research access\n",
    "    allow_delegation=True,\n",
    "    llm_model=openai_model\n",
    ")\n",
    "\n",
    "# Research Summarizer - Now compares proposed models to existing astrophysical theories\n",
    "research_summarizer = Agent(\n",
    "    role=\"Research Validation Agent\",\n",
    "    goal=\"Find relevant research and evaluate how well theoretical models align with existing astrophysics literature.\",\n",
    "    backstory=\"You specialize in astrophysics literature review, helping determine whether proposed models align with or challenge existing theories.\",\n",
    "    verbose=True,\n",
    "    tools=[fetch_astronomy_papers],  # May need an evaluation tool later\n",
    "    allow_delegation=True,\n",
    "    llm_model=openai_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [data_ingestor, gravitational_wave_analyst, anomaly_detector, theoretical_modeler, research_summarizer]\n",
    "\n",
    "for agent in agents:\n",
    "    print(f\"{agent.role} is using LLM: {agent.llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Agent Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks for each agent\n",
    "\n",
    "# Define validation task\n",
    "data_validation_task = Task(\n",
    "    description=\"Validate event IDs and check for missing fields before processing.\",\n",
    "    agent=data_validator,\n",
    "    expected_output=\"List of valid events and rejected events with reasons.\"\n",
    ")\n",
    "\n",
    "# Data ingestion runs first and async\n",
    "data_ingestion_task = Task(\n",
    "    description=\"Fetch the latest gravitational wave and exoplanet data from various sources.\",\n",
    "    agent=data_ingestor,\n",
    "    expected_output=\"A structured dataset of gravitational wave events and exoplanet data.\",\n",
    "    async_execution=True  # Allow parallel execution\n",
    ")\n",
    "\n",
    "# GW analysis processes ingested data\n",
    "gw_analysis_task = Task(\n",
    "    description=\"Analyze gravitational wave events to extract key astrophysical parameters.\",\n",
    "    agent=gravitational_wave_analyst,\n",
    "    expected_output=\"Key parameters extracted from gravitational wave data.\",\n",
    "    context=[data_ingestion_task],  # Waits for ingestion\n",
    "    async_execution=False\n",
    ")\n",
    "\n",
    "# Anomaly detection runs after analysis\n",
    "anomaly_detection_task = Task(\n",
    "    description=\"Identify anomalies in the gravitational wave dataset.\",\n",
    "    agent=anomaly_detector,\n",
    "    expected_output=\"List of anomalies with descriptions.\",\n",
    "    context=[gw_analysis_task],  # Depends on GW analysis results\n",
    "    async_execution=False\n",
    ")\n",
    "\n",
    "# Theoretical modeling depends on anomaly detection\n",
    "theoretical_modeling_task = Task(\n",
    "    description=\"Develop theoretical explanations for gravitational wave anomalies.\",\n",
    "    agent=theoretical_modeler,\n",
    "    expected_output=\"New theoretical models explaining anomalies.\",\n",
    "    context=[anomaly_detection_task, data_ingestion_task],  # Uses both anomalies & raw data\n",
    "    async_execution=False\n",
    ")\n",
    "\n",
    "# Research Summarization happens last\n",
    "research_summary_task = Task(\n",
    "    description=\"Find research papers relevant to the new theoretical models.\",\n",
    "    agent=research_summarizer,\n",
    "    expected_output=\"Summaries of supporting or contradicting research papers.\",\n",
    "    context=[theoretical_modeling_task],  # Only starts after models are generated\n",
    "    async_execution=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the crew and assign tasks\n",
    "astrophysics_crew = Crew(\n",
    "    agents=[data_ingestor, gravitational_wave_analyst, anomaly_detector, theoretical_modeler, research_summarizer],\n",
    "    tasks=[data_ingestion_task, gw_analysis_task, anomaly_detection_task, theoretical_modeling_task, research_summary_task],\n",
    "    verbose=False,\n",
    "    process=Process.sequential  # Can also use Process.hierarchical for more complex workflows\n",
    ")\n",
    "\n",
    "# Run the crew\n",
    "try:\n",
    "    print(\"Starting the Astrophysics Research Crew...\")\n",
    "    print(\"This may take some time to complete all tasks.\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Execute the workflow\n",
    "    results = astrophysics_crew.kickoff()\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    print(\"Crew execution completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during crew execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract structured results from API response\n",
    "task_outputs = results[\"tasks_output\"]\n",
    "\n",
    "# Initialize formatted results\n",
    "formatted_results = \"\"\"\n",
    "# ASTROPHYSICS ANALYSIS REPORT  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Loop through task outputs and extract results\n",
    "for task in task_outputs:\n",
    "    agent_name = task[\"agent\"]\n",
    "    task_result = task[\"raw\"] if task[\"raw\"] else \"No result available.\"\n",
    "\n",
    "    formatted_results += f\"\"\"\n",
    "\n",
    "## {agent_name} Analysis  \n",
    "\n",
    "{task_result}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save results as a structured text file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"astrophysics_report_{timestamp}.md\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(formatted_results)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse and extract data from the crew results\n",
    "def extract_data_from_results(results):\n",
    "    try:\n",
    "        # Extract gravitational wave events\n",
    "        gw_events = []\n",
    "        anomalies = []\n",
    "        theoretical_models = []\n",
    "        \n",
    "        # Parse the results\n",
    "        for task_result in results.values():\n",
    "            if isinstance(task_result, str):\n",
    "                # Try to parse JSON from string\n",
    "                try:\n",
    "                    parsed_data = json.loads(task_result)\n",
    "                    if isinstance(parsed_data, list) and len(parsed_data) > 0:\n",
    "                        # Determine type of data based on keys\n",
    "                        first_item = parsed_data[0]\n",
    "                        if 'event_id' in first_item and 'mass_1_source' in first_item:\n",
    "                            gw_events.extend(parsed_data)\n",
    "                        elif 'event_id' in first_item and 'anomaly_type' in first_item:\n",
    "                            anomalies.extend(parsed_data)\n",
    "                        elif 'event_id' in first_item and 'model_name' in first_item:\n",
    "                            theoretical_models.extend(parsed_data)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return {\n",
    "            'gw_events': gw_events,\n",
    "            'anomalies': anomalies,\n",
    "            'theoretical_models': theoretical_models\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from results: {str(e)}\")\n",
    "        return {\n",
    "            'gw_events': [],\n",
    "            'anomalies': [],\n",
    "            'theoretical_models': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gravitational wave events and anomalies\n",
    "def visualize_gw_data(extracted_data):\n",
    "    gw_events = extracted_data.get('gw_events', [])\n",
    "    anomalies = extracted_data.get('anomalies', [])\n",
    "    \n",
    "    if not gw_events:\n",
    "        print(\"No gravitational wave event data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrames\n",
    "    gw_df = pd.DataFrame(gw_events)\n",
    "    \n",
    "    # Convert columns to numeric\n",
    "    numeric_cols = ['mass_1_source', 'mass_2_source', 'luminosity_distance', 'network_snr']\n",
    "    for col in numeric_cols:\n",
    "        if col in gw_df.columns:\n",
    "            gw_df[col] = pd.to_numeric(gw_df[col], errors='coerce')\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Mass plot (mass_1 vs mass_2)\n",
    "    if 'mass_1_source' in gw_df.columns and 'mass_2_source' in gw_df.columns:\n",
    "        ax1 = fig.add_subplot(2, 2, 1)\n",
    "        ax1.scatter(gw_df['mass_1_source'], gw_df['mass_2_source'], alpha=0.7, s=50)\n",
    "        \n",
    "        # Mark anomalies if available\n",
    "        anomaly_events = [a['event_id'] for a in anomalies if 'High mass ratio' in a.get('anomaly_type', '')]\n",
    "        anomaly_df = gw_df[gw_df['event_id'].isin(anomaly_events)]\n",
    "        if not anomaly_df.empty:\n",
    "            ax1.scatter(anomaly_df['mass_1_source'], anomaly_df['mass_2_source'], \n",
    "                      color='red', s=100, label='Mass Anomalies')\n",
    "        \n",
    "        ax1.set_xlabel('Primary Mass (Solar masses)')\n",
    "        ax1.set_ylabel('Secondary Mass (Solar masses)')\n",
    "        ax1.set_title('Mass Distribution of Black Hole Mergers')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # 2. Distance vs SNR plot\n",
    "    if 'luminosity_distance' in gw_df.columns and 'network_snr' in gw_df.columns:\n",
    "        ax2 = fig.add_subplot(2, 2, 2)\n",
    "        sc = ax2.scatter(gw_df['luminosity_distance'], gw_df['network_snr'], \n",
    "                       alpha=0.7, s=50, c=gw_df['mass_1_source'], cmap='viridis')\n",
    "        \n",
    "        # Mark anomalies if available\n",
    "        snr_anomaly_events = [a['event_id'] for a in anomalies if 'High SNR' in a.get('anomaly_type', '')]\n",
    "        dist_anomaly_events = [a['event_id'] for a in anomalies if 'Distant event' in a.get('anomaly_type', '')]\n",
    "        \n",
    "        snr_anomaly_df = gw_df[gw_df['event_id'].isin(snr_anomaly_events)]\n",
    "        dist_anomaly_df = gw_df[gw_df['event_id'].isin(dist_anomaly_events)]\n",
    "        \n",
    "        if not snr_anomaly_df.empty:\n",
    "            ax2.scatter(snr_anomaly_df['luminosity_distance'], snr_anomaly_df['network_snr'], \n",
    "                      color='red', s=100, marker='*', label='SNR Anomalies')\n",
    "            \n",
    "        if not dist_anomaly_df.empty:\n",
    "            ax2.scatter(dist_anomaly_df['luminosity_distance'], dist_anomaly_df['network_snr'], \n",
    "                      color='orange', s=100, marker='s', label='Distance Anomalies')\n",
    "        \n",
    "        cbar = plt.colorbar(sc, ax=ax2)\n",
    "        cbar.set_label('Primary Mass (Solar masses)')\n",
    "        ax2.set_xlabel('Luminosity Distance (Mpc)')\n",
    "        ax2.set_ylabel('Network SNR')\n",
    "        ax2.set_title('Distance vs. Signal-to-Noise Ratio')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. Anomaly count by type\n",
    "    if anomalies:\n",
    "        ax3 = fig.add_subplot(2, 2, 3)\n",
    "        anomaly_types = [a['anomaly_type'] for a in anomalies]\n",
    "        anomaly_counts = {}\n",
    "        for atype in anomaly_types:\n",
    "            if atype in anomaly_counts:\n",
    "                anomaly_counts[atype] += 1\n",
    "            else:\n",
    "                anomaly_counts[atype] = 1\n",
    "        \n",
    "        bars = ax3.bar(anomaly_counts.keys(), anomaly_counts.values(), color='purple')\n",
    "        ax3.set_xlabel('Anomaly Type')\n",
    "        ax3.set_ylabel('Count')\n",
    "        ax3.set_title('Distribution of Anomaly Types')\n",
    "        plt.setp(ax3.get_xticklabels(), rotation=30, ha='right')\n",
    "        \n",
    "        # Add counts on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                   f'{height:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Model distribution\n",
    "    theoretical_models = extracted_data.get('theoretical_models', [])\n",
    "    if theoretical_models:\n",
    "        ax4 = fig.add_subplot(2, 2, 4)\n",
    "        model_names = [m['model_name'] for m in theoretical_models]\n",
    "        model_counts = {}\n",
    "        for model in model_names:\n",
    "            if model in model_counts:\n",
    "                model_counts[model] += 1\n",
    "            else:\n",
    "                model_counts[model] = 1\n",
    "        \n",
    "        pie_wedges, texts, autotexts = ax4.pie(model_counts.values(), labels=model_counts.keys(), \n",
    "                                            autopct='%1.1f%%', startangle=90,\n",
    "                                            colors=plt.cm.tab10.colors[:len(model_counts)])\n",
    "        ax4.set_title('Distribution of Theoretical Models')\n",
    "        # Make text easier to read\n",
    "        for text in texts + autotexts:\n",
    "            text.set_fontsize(9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary of findings\n",
    "    print(f\"Total gravitational wave events analyzed: {len(gw_events)}\")\n",
    "    print(f\"Total anomalies detected: {len(anomalies)}\")\n",
    "    print(f\"Total theoretical models proposed: {len(theoretical_models)}\")\n",
    "    print(\"\\nTop anomalies:\")\n",
    "    for i, anomaly in enumerate(anomalies[:3]):\n",
    "        print(f\"  {i+1}. {anomaly.get('anomaly_type', 'Unknown')} in event {anomaly.get('event_id', 'Unknown')}: {anomaly.get('details', '')}\")\n",
    "    \n",
    "    print(\"\\nProposed theoretical models:\")\n",
    "    for i, model in enumerate(theoretical_models[:3]):\n",
    "        print(f\"  {i+1}. {model.get('model_name', 'Unknown')} for event {model.get('event_id', 'Unknown')}: {model.get('description', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the data\n",
    "# Note: In a real run, you would use the actual results from the crew\n",
    "# Here we're using simulated or example data if needed\n",
    "\n",
    "try:\n",
    "    # Extract data from crew results\n",
    "    extracted_data = extract_data_from_results(results)\n",
    "    \n",
    "    # If no real data is available, you can use example data for testing visualization\n",
    "    if not extracted_data['gw_events']:\n",
    "        print(\"No gravitational wave data found in results. Using example data for visualization...\")\n",
    "        \n",
    "        # Example GW events\n",
    "        example_gw_events = [\n",
    "            {'event_id': 'GW150914', 'mass_1_source': 36.2, 'mass_2_source': 29.1, 'luminosity_distance': 410, 'network_snr': 24.4},\n",
    "            {'event_id': 'GW151226', 'mass_1_source': 14.2, 'mass_2_source': 7.5, 'luminosity_distance': 440, 'network_snr': 13.1},\n",
    "            {'event_id': 'GW170104', 'mass_1_source': 31.2, 'mass_2_source': 19.4, 'luminosity_distance': 880, 'network_snr': 13.0},\n",
    "            {'event_id': 'GW170814', 'mass_1_source': 30.5, 'mass_2_source': 25.3, 'luminosity_distance': 540, 'network_snr': 18.3},\n",
    "            {'event_id': 'GW190521', 'mass_1_source': 85.0, 'mass_2_source': 66.0, 'luminosity_distance': 5300, 'network_snr': 14.7},\n",
    "            {'event_id': 'GW190814', 'mass_1_source': 23.2, 'mass_2_source': 2.6, 'luminosity_distance': 240, 'network_snr': 25.0},\n",
    "            {'event_id': 'GW191109', 'mass_1_source': 65.0, 'mass_2_source': 47.0, 'luminosity_distance': 1400, 'network_snr': 15.6},\n",
    "            {'event_id': 'GW200115', 'mass_1_source': 5.7, 'mass_2_source': 1.5, 'luminosity_distance': 300, 'network_snr': 11.3}\n",
    "        ]\n",
    "        \n",
    "        # Example anomalies\n",
    "        example_anomalies = [\n",
    "            {'event_id': 'GW190814', 'anomaly_type': 'High mass ratio', 'details': 'Mass ratio: 8.92 (mass_1: 23.2, mass_2: 2.6)', 'significance': 'high'},\n",
    "            {'event_id': 'GW190521', 'anomaly_type': 'Distant event', 'details': 'Distance: 5300.00 Mpc', 'significance': 'high'},\n",
    "            {'event_id': 'GW150914', 'anomaly_type': 'High SNR', 'details': 'SNR: 24.40', 'significance': 'medium'},\n",
    "            {'event_id': 'GW200115', 'anomaly_type': 'NSBH merger', 'details': 'Neutron star - black hole merger', 'significance': 'high'}\n",
    "        ]\n",
    "        \n",
    "        # Example theoretical models\n",
    "        example_theoretical_models = [\n",
    "            {'event_id': 'GW190814', 'model_name': 'Hierarchical Merger Model', 'description': 'This event may represent a hierarchical merger, where one of the black holes is itself a product of a previous merger.'},\n",
    "            {'event_id': 'GW190521', 'model_name': 'Primordial Black Hole Model', 'description': 'This distant event may involve primordial black holes formed in the early universe rather than from stellar collapse.'},\n",
    "            {'event_id': 'GW150914', 'model_name': 'Binary Black Hole Optimal Orientation Model', 'description': 'This event may have an unusually high SNR due to optimal orientation with respect to our detectors.'},\n",
    "            {'event_id': 'GW200115', 'model_name': 'Neutron Star Equation of State Model', 'description': 'This NSBH merger provides constraints on neutron star equation of state.'}\n",
    "        ]\n",
    "        \n",
    "        extracted_data = {\n",
    "            'gw_events': example_gw_events,\n",
    "            'anomalies': example_anomalies,\n",
    "            'theoretical_models': example_theoretical_models\n",
    "        }\n",
    "    \n",
    "    # Visualize the data\n",
    "    visualize_gw_data(extracted_data)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a multi-agent AI system for astrophysical research, focusing on gravitational wave data analysis. The system:\n",
    "\n",
    "1. Fetches real gravitational wave data from public sources\n",
    "2. Analyzes the data to identify anomalies and unusual patterns\n",
    "3. Generates theoretical models to explain the anomalies\n",
    "4. Correlates findings with existing research\n",
    "5. Visualizes results in an informative way\n",
    "\n",
    "This approach allows AI agents to autonomously perform complex astrophysical research tasks that would normally require significant human effort and expertise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
